# Snowflake-AI
 Contains the AI queries for Snowflake

# 1. How to upload LLM models into Snowflake
You may upload your own LLM models in Snowflake stage. In my case, there was a models.zip (410 MB) file containing - tokenizer, pytorch_model.bin among other jsons. So I had to upload the zip to snowflake stage using SnowSQL. Why? because using the web UI, only 250 MB files can be uploaded. So used the PUT command to upload the models.zip
After this, you need to unzip the file in Snowflake notebook's working directory. Then you can use the pytorch_model.bin in your training code

# 2. Text2SQL AI chatbot 
Technologies: Snowflake Cortex, Python, Streamlit <br>
Text2SQL chatbot takes plain text english query, transforms that into a snowflake-compatible SQL & returns results <br>

**Features:** <br>
1. Shows the interpretted Snowflake-compatible SQL (generated by LLM) for the given plain text query
2. Maintains conversational context history for past 'n' messages. 'n' is configurable

**Setup Instructions:** <br>
1. Create demo_sm.yaml file from the Snowsight UI (AI & ML > Cortex Analyst > Try > Create new). Select the tables & columns you want to have queries run on
2. The above step will also ask you to store the generated yaml file in a stage location. This stage location is used in the python code
<br><br>

**Run Instructions:** <br>
1. python -m venv streamlit-env
2. source streamlit-env/bin/activate
3. pip install -r requirements.txt
4. streamlit run analyst_demo.py
